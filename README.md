# GoMesh - Service Mesh Implementation

A high-performance, distributed Service Mesh with a gRPC Control Plane built from scratch in Go.

## Current Status: Phase 2 Part 4 - Distributed Tracing ‚úÖ

### What's Complete:

- ‚úÖ **Phase 1**: Basic reverse proxy with graceful shutdown
- ‚úÖ **Phase 2 Part 1**: Structured JSON logging with Zap and logging middleware
- ‚úÖ **Phase 2 Part 2**: Prometheus metrics with `/metrics` endpoint
- ‚úÖ **Phase 2 Part 3**: Recovery middleware and middleware chaining
- ‚úÖ **Phase 2 Part 4**: Distributed tracing with unique trace IDs

### Next Up:

- üìç **Phase 3**: gRPC Control Plane for dynamic configuration

## Project Structure

```
GoMesh/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îú‚îÄ‚îÄ proxy/              # Main proxy binary
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.go
‚îÇ   ‚îî‚îÄ‚îÄ backend/            # Test backend service
‚îÇ       ‚îî‚îÄ‚îÄ main.go
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îú‚îÄ‚îÄ logging/            # Structured logging (Phase 2 Part 1)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging.go      # Zap logger wrapper
‚îÇ   ‚îú‚îÄ‚îÄ tracing/            # Distributed tracing (Phase 2 Part 4)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tracer.go       # Trace ID generation and propagation
‚îÇ   ‚îî‚îÄ‚îÄ proxy/              # Proxy package
‚îÇ       ‚îú‚îÄ‚îÄ config.go       # Configuration loader
‚îÇ       ‚îú‚îÄ‚îÄ handler.go      # Reverse proxy logic
‚îÇ       ‚îú‚îÄ‚îÄ middleware.go   # All middleware (logging, metrics, tracing, recovery)
‚îÇ       ‚îú‚îÄ‚îÄ metrics.go      # Prometheus metrics (Phase 2 Part 2)
‚îÇ       ‚îî‚îÄ‚îÄ server.go       # HTTP server
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ proxy.yaml          # Proxy configuration
‚îú‚îÄ‚îÄ go.mod
‚îî‚îÄ‚îÄ go.sum
```

## How to Run

### Step 1: Install Dependencies

```bash
cd gomesh
go mod download
```

### Step 2: Start the Backend Service

In one terminal:

```bash
go run cmd/backend/main.go
```

You should see:

```
[BACKEND] Starting test backend on :3000
[BACKEND] Ready to receive requests from the proxy
```

### Step 3: Start the Proxy

In another terminal:

```bash
go run cmd/proxy/main.go
```

You should see (structured JSON logs):

```json
{"level":"info","timestamp":"2026-01-25T...","msg":"Loading configuration from: config/proxy.yaml"}
{"level":"info","timestamp":"2026-01-25T...","msg":"GoMesh Proxy starting on port 8000"}
{"level":"info","timestamp":"2026-01-25T...","msg":"Forwarding all traffic to: http://localhost:3000"}
{"level":"info","timestamp":"2026-01-25T...","msg":"Press Ctrl+C to stop"}
```

### Step 4: Test It!

In a third terminal:

```bash
# Send a request to the proxy
curl http://localhost:8000/api/users

# You should see:
# {
#   "message": "Hello from the backend service!",
#   "timestamp": "2026-01-24T...",
#   "path": "/api/users",
#   "method": "GET"
# }
```

### Step 5: Check Metrics!

```bash
# View Prometheus metrics
curl http://localhost:8000/metrics

# You'll see metrics like:
# gomesh_requests_total{service="backend",status="2xx"} 1
# gomesh_request_duration_seconds_bucket{service="backend",le="0.05"} 1
# gomesh_requests_in_flight 0
```

### Step 6: Test Recovery Middleware!

```bash
# Trigger a panic in the backend (proxy should handle it gracefully)
curl http://localhost:8000/panic

# You'll see:
# - Client receives: "Internal Server Error" (HTTP 500)
# - Proxy logs the panic with full stack trace
# - Proxy KEEPS RUNNING (doesn't crash!) ‚úÖ
```

### Step 7: Test Distributed Tracing!

```bash
# Make a request and check the trace ID in the response headers
curl -v http://localhost:8000/api/users

# You'll see in the response headers:
# X-Trace-ID: a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6

# The response body also includes the trace ID:
# {
#   "message": "Hello from the backend service!",
#   "trace_id": "a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6",
#   ...
# }

# You can also provide your own trace ID:
curl -H "X-Trace-ID: my-custom-trace-123" http://localhost:8000/api/users

# The backend will echo it back in logs and response
```

## What's Happening?

```
Your Request
    ‚Üì
    ‚Üì (HTTP GET /api/users)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GoMesh Proxy   ‚îÇ  ‚Üê Listening on :8000
‚îÇ  (localhost)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚Üì (Forwards to backend)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend App    ‚îÇ  ‚Üê Listening on :3000
‚îÇ  (localhost)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚Üì (Returns JSON)
    ‚Üì
Your Response
```

## Watch the Logs

In the proxy terminal, you'll see structured JSON logs:

```json
{"level":"info","timestamp":"2026-01-25T10:30:05.200Z","msg":"Request starter","trace_id":"none","method":"GET","path":"/api/users","remote_addr":"127.0.0.1:53242"}
{"level":"info","timestamp":"2026-01-25T10:30:05.245Z","msg":"request completed","method":"GET","path":"/api/users","status":200,"latency_ms":"45ms","trace_id":"none"}
```

In the backend terminal, you'll see:

```
[BACKEND] Received: GET /api/users
```

### What the Logs Tell You:

- **trace_id**: Unique 128-bit identifier for tracking requests across services
- **method**: HTTP method (GET, POST, etc.)
- **path**: Request path
- **status**: HTTP status code (200, 404, 500, etc.)
- **latency_ms**: Time taken to process the request
- **timestamp**: ISO8601 formatted timestamp
- **remote_addr**: Client IP address

## Configuration

Edit `config/proxy.yaml` to change:

- Proxy listen port (default: 8000)
- Backend host/port (default: localhost:3000)
- Timeouts

## What We've Learned

### Phase 1: Basic Reverse Proxy ‚úÖ

- ‚úÖ **Go HTTP Server** - Using `net/http`
- ‚úÖ **Reverse Proxy** - Using `httputil.ReverseProxy`
- ‚úÖ **Configuration** - YAML parsing with `gopkg.in/yaml.v3`
- ‚úÖ **Graceful Shutdown** - Signal handling with `os/signal`
- ‚úÖ **Project Structure** - Standard Go project layout
- ‚úÖ **Goroutines** - Running server in background
- ‚úÖ **Channels** - Communication between goroutines

### Phase 2 Part 1: Structured Logging ‚úÖ

- ‚úÖ **Zap Logger** - High-performance structured logging with `go.uber.org/zap`
- ‚úÖ **Middleware Pattern** - Function wrapping: `func(http.Handler) http.Handler`
- ‚úÖ **Interface Wrapping** - Custom `responseWriter` wraps `http.ResponseWriter`
- ‚úÖ **Request/Response Tracking** - Capturing status codes and latency
- ‚úÖ **Defer Pattern** - `defer logger.Sync()` ensures cleanup
- ‚úÖ **Structured Fields** - JSON logs with typed fields (method, path, status, latency)

### Phase 2 Part 2: Prometheus Metrics ‚úÖ

- ‚úÖ **Prometheus Client** - Using `github.com/prometheus/client_golang`
- ‚úÖ **Metric Types** - Counters (requests_total), Histograms (duration), Gauges (in_flight)
- ‚úÖ **Metric Labels** - Multi-dimensional metrics (service, status, error_type)
- ‚úÖ **promauto Package** - Automatic registration with default registry
- ‚úÖ **HTTP Multiplexer** - Using `http.NewServeMux()` for multiple endpoints
- ‚úÖ **Middleware Chaining** - Metrics ‚Üí Logging ‚Üí Proxy handler stack
- ‚úÖ **/metrics Endpoint** - Standard Prometheus scraping endpoint

### Phase 2 Part 3: Advanced Middleware ‚úÖ

- ‚úÖ **Panic Recovery** - Using `defer` + `recover()` to catch runtime panics
- ‚úÖ **Stack Traces** - `runtime/debug.Stack()` for debugging panics
- ‚úÖ **Resilient Design** - Isolated request failures don't crash the entire proxy
- ‚úÖ **Middleware Chaining** - Variadic function pattern for composable middleware
- ‚úÖ **Order-Aware Composition** - Reverse loop to apply middleware in correct order
- ‚úÖ **Graceful Degradation** - Return 500 on panic, log details, keep serving

### Phase 2 Part 4: Distributed Tracing ‚úÖ

- ‚úÖ **Trace ID Generation** - Using `crypto/rand` for cryptographically secure 128-bit IDs
- ‚úÖ **Header Propagation** - `X-Trace-ID` header forwarded to backend services
- ‚úÖ **Request Correlation** - Track requests across multiple services with unique IDs
- ‚úÖ **Response Headers** - Return trace ID to clients for debugging
- ‚úÖ **Logging Integration** - All logs include trace_id field for request correlation
- ‚úÖ **Fallback Mechanism** - Timestamp-based fallback if crypto/rand fails

## Next Steps: Phase 3 - gRPC Control Plane

### Phase 3 Part 1: gRPC API Definition üìç NEXT

- Define Protocol Buffer schemas for configuration
- Generate Go code from .proto files
- Design control plane API (GetConfig, UpdateConfig, etc.)

### Phase 3 Part 2: Control Plane Server

- Implement gRPC server for control plane
- Dynamic configuration management
- Hot reload without proxy restart

## Troubleshooting

**Port already in use?**

```bash
# Find what's using port 8000
lsof -i :8000

# Or change the port in config/proxy.yaml
```

**Can't connect to backend?**

```bash
# Make sure backend is running
curl http://localhost:3000/health
```

## Commands Cheat Sheet

```bash
# Run proxy
go run cmd/proxy/main.go

# Run proxy with custom config
go run cmd/proxy/main.go -config /path/to/config.yaml

# Build the binary
go build -o bin/proxy cmd/proxy/main.go

# Run the binary
./bin/proxy

# Test the proxy
curl -v http://localhost:8000/test
curl -X POST http://localhost:8000/api/data -d '{"key":"value"}'
```

---

## How the Middleware Stack Works

Understanding the request flow through our middleware chain:

```
Request from Client (with optional X-Trace-ID header)
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RecoveryMiddleware       ‚îÇ  ‚Üê OUTERMOST: Catches ALL panics (defer/recover)
‚îÇ  (panic safety)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TracingMiddleware        ‚îÇ  ‚Üê Generates/extracts trace ID
‚îÇ  (distributed tracing)    ‚îÇ     Sets X-Trace-ID header in request & response
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MetricsMiddleware        ‚îÇ  ‚Üê Increments in-flight counter, starts timer
‚îÇ  (observability)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LoggingMiddleware        ‚îÇ  ‚Üê Logs "request started" with trace_id
‚îÇ  (structured logging)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ProxyHandler             ‚îÇ  ‚Üê Forwards to backend with X-Trace-ID header
‚îÇ  (httputil.ReverseProxy)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend Service :3000    ‚îÇ  ‚Üê Receives trace ID, echoes in logs & response
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LoggingMiddleware        ‚îÇ  ‚Üê Logs "request completed" with trace_id
‚îÇ  (calculates latency)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MetricsMiddleware        ‚îÇ  ‚Üê Records metrics (counter, histogram, gauge)
‚îÇ  (records metrics)        ‚îÇ     Decrements in-flight counter
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TracingMiddleware        ‚îÇ  ‚Üê X-Trace-ID already set in response header
‚îÇ  (trace ID in response)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RecoveryMiddleware       ‚îÇ  ‚Üê If panic occurred, catches it here
‚îÇ  (returns 500 if panic)   ‚îÇ     Logs with stack trace & trace_id, returns 500
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
Response to Client (with X-Trace-ID header)
```

### Middleware Order Matters!

The order of middleware is critical:
1. **Recovery** (outermost) - Must catch panics from ALL inner middleware
2. **Tracing** - Generate/extract trace ID early so all inner middleware can use it
3. **Metrics** - Track all requests (even if they panic)
4. **Logging** - Log all requests with trace ID (even if they panic)
5. **Proxy Handler** - The actual reverse proxy logic

### The Chain Function

Using the Chain helper makes middleware composition clean and readable:

```go
// Before: Manual nesting (hard to read)
handler = RecoveryMiddleware(logger,
    TracingMiddleware(
        MetricsMiddleware(metrics,
            LoggingMiddleware(logger, handler))))

// After: Chain function (clean & clear)
handler = Chain(handler,
    RecoveryMiddleware(logger),    // First = outermost
    TracingMiddleware(),           // Second
    MetricsMiddleware(metrics),    // Third
    LoggingMiddleware(logger),     // Last = innermost
)
```

**How Chain works:**

```go
func Chain(handler http.Handler, middlewares ...func(http.Handler) http.Handler) http.Handler {
    // Apply in reverse so first middleware becomes outermost
    for i := len(middlewares) - 1; i >= 0; i-- {
        handler = middlewares[i](handler)
    }
    return handler
}
```

This produces: `Recovery(Tracing(Metrics(Logging(handler))))`

### Before vs After Logging

**Phase 1 (Basic Logging):**

```
[INFO] Forwarding: GET /api/users ‚Üí localhost:3000
```

**Phase 2 Part 1-4 (Structured Logging + Tracing):**

```json
{"level":"info","timestamp":"2026-01-26T10:30:05.200Z","msg":"Request starter","trace_id":"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6","method":"GET","path":"/api/users","remote_addr":"127.0.0.1:53242"}
{"level":"info","timestamp":"2026-01-26T10:30:05.245Z","msg":"request completed","method":"GET","path":"/api/users","status":200,"latency_ms":"45ms","trace_id":"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6"}
```

The structured logs are:

- **Machine-readable** - Easy to parse and analyze
- **Searchable** - Query by any field (status=500, latency>100ms, etc.)
- **Standardized** - Consistent format across all services

### Prometheus Metrics

After sending some requests, check the `/metrics` endpoint:

```bash
curl http://localhost:8000/metrics
```

**Key Metrics Available:**

```prometheus
# Total number of requests (labeled by service and status bucket)
gomesh_requests_total{service="backend",status="2xx"} 5
gomesh_requests_total{service="backend",status="4xx"} 1
gomesh_requests_total{service="backend",status="5xx"} 0

# Request duration histogram (shows latency distribution)
gomesh_request_duration_seconds_bucket{service="backend",le="0.005"} 3
gomesh_request_duration_seconds_bucket{service="backend",le="0.01"} 5
gomesh_request_duration_seconds_bucket{service="backend",le="0.05"} 5
gomesh_request_duration_seconds_sum{service="backend"} 0.023
gomesh_request_duration_seconds_count{service="backend"} 5

# Number of requests currently being processed
gomesh_requests_in_flight 0

# Total errors (labeled by service and error type)
gomesh_errors_total{service="backend",error_type="timeout"} 0
```

**What These Metrics Tell You:**

- **requests_total**: Count requests by status code (2xx, 3xx, 4xx, 5xx)
- **request_duration_seconds**: Latency percentiles (p50, p95, p99) for SLA tracking
- **requests_in_flight**: Current load on the proxy
- **errors_total**: Error counts by type for alerting

### Testing Recovery Middleware

The backend has a `/panic` endpoint that intentionally triggers a panic to test recovery:

```bash
# Trigger a panic
curl http://localhost:8000/panic
```

**What happens:**

1. **Client receives:**
   ```
   Internal Server Error
   ```
   HTTP status: 500

2. **Proxy logs show:**
   ```json
   {
     "level": "error",
     "msg": "panic recovered",
     "error": "intentional panic for testing recovery middleware!",
     "path": "/panic",
     "method": "GET",
     "stack": "goroutine 123 [running]:\nruntime/debug.Stack()..."
   }
   ```

3. **Proxy keeps running!** ‚úÖ
   - Other requests still work
   - No downtime
   - Panic isolated to single request

**Without recovery middleware, the entire proxy would crash.** This is the difference between:
- ‚ùå One bad request ‚Üí entire proxy down ‚Üí manual restart
- ‚úÖ One bad request ‚Üí 500 error ‚Üí logged ‚Üí proxy still running

### Distributed Tracing with Trace IDs

Every request gets a unique trace ID that follows it through your entire system:

```bash
# Make a request
curl -v http://localhost:8000/api/users
```

**1. Proxy receives request:**
```json
{"level":"info","msg":"Request starter","trace_id":"a1b2c3d4...","method":"GET","path":"/api/users"}
```

**2. Backend receives trace ID:**
```
[BACKEND] Received: GET /api/users | Trace-ID: a1b2c3d4...
```

**3. Backend returns response with trace ID:**
```json
{
  "message": "Hello from the backend service!",
  "trace_id": "a1b2c3d4...",
  ...
}
```

**4. Client receives trace ID in header:**
```
< X-Trace-ID: a1b2c3d4...
```

**Why This Matters:**

In a microservices architecture with multiple services:
```
Client ‚Üí Proxy ‚Üí Service A ‚Üí Service B ‚Üí Service C
```

**Without trace IDs:**
- ‚ùå Logs scattered across services
- ‚ùå Can't correlate which logs belong to same request
- ‚ùå Hard to debug issues spanning multiple services
- ‚ùå No visibility into request flow

**With trace IDs:**
- ‚úÖ One ID tracks request through entire system
- ‚úÖ Search logs for `trace_id: a1b2c3d4...` across all services
- ‚úÖ See complete request journey
- ‚úÖ Identify exactly where failures occurred
- ‚úÖ Measure end-to-end latency

**Example debugging scenario:**
```bash
# User reports error, provides trace ID from response
# Search all logs for this trace ID:

# Proxy logs:
{"trace_id":"abc123","msg":"Request starter","path":"/api/order"}
{"trace_id":"abc123","msg":"request completed","status":500}

# Service A logs:
{"trace_id":"abc123","msg":"processing order","order_id":42}
{"trace_id":"abc123","msg":"calling payment service"}

# Service B logs:
{"trace_id":"abc123","error":"payment declined"}  ‚Üê Found the issue!
```

This is the foundation of observability in distributed systems!

---

## Complete Roadmap

### ‚úÖ Phase 1: Basic Reverse Proxy (Complete)

- HTTP reverse proxy
- YAML configuration
- Graceful shutdown

### ‚úÖ Phase 2 Part 1: Structured Logging (Complete)

- Zap logger integration
- Logging middleware
- Request/response tracking

### ‚úÖ Phase 2 Part 2: Prometheus Metrics (Complete)

- Metrics package with Prometheus client
- `/metrics` endpoint for scraping
- Request counters, histograms, and gauges
- Metrics middleware for automatic tracking

### ‚úÖ Phase 2 Part 3: Advanced Middleware (Complete)

- Middleware chaining helper with Chain() function
- Recovery middleware with panic handling
- Stack trace logging for debugging
- Resilient proxy that doesn't crash on panics

### ‚úÖ Phase 2 Part 4: Distributed Tracing (Complete)

- Cryptographically secure trace ID generation (128-bit)
- X-Trace-ID header propagation to backend services
- Trace ID included in all logs for request correlation
- Response headers include trace ID for client debugging

### Phase 3: Control Plane with gRPC

- gRPC API definition
- Control plane server
- Dynamic configuration
- Hot reload

### Phase 4: Service Discovery & Load Balancing

- Service registry
- Round-robin load balancing
- Health checking

### Phase 5: Production Features

- mTLS encryption
- Circuit breaker
- Rate limiting
- JWT validation
